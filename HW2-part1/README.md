# HW2-Part1
###  Problem 1.
Two Normal Distributions
Consider a single scalar feature ğ‘¥, and two classes c0, c1;
each with equal prior probability: P[c0] = P[c1] = $\frac{1}{2}$.
Thus,
$$P[c1|ğ‘¥] = \frac{P[c1]P[x|c1]}{P[ğ‘¥]} = \frac{P[ğ‘¥|c1]}{2P[ğ‘¥]} \propto P[ğ‘¥|c1]$$
Assume the probability distribution of a feature ğ‘¥ depends only on the class.
In particular;$x~N(-1,0)$ for class c0, and x~N(+1, 0) for class c1.

Let ğ‘…(ğ‘¥) denote the probability of P[ğ‘¥|c1] as a function of ğ‘¥,

Question:

Derive ğ‘…(ğ‘¥) and express it as:
1. the (sigmoid) logistic function. ğ‘…(ğ‘¥) = ğœ(?).
2. Softmax (multinomial logistic function). [1 âˆ’ ğ‘…(ğ‘¥), ğ‘…(ğ‘¥)] = softmax(?, ?)
Discuss how the form of ğ‘…(ğ‘¥) relates to logistic regression.

### Problem 2.
Question:

Prove the following theorem.

Theorem 1: A general property of the logistic function is:

$âˆ€ğ‘$ $\underset{b}{\text{argmax}}$ $ğ‘ğœ(ğ‘ + ğ‘) ğœ(ğ‘ âˆ’ ğ‘) = 0$

Where ğœ(ğ‘¥) denotes the logistic function: 1/(1 + exp(âˆ’ğ‘¥))

In other words if ğ‘ is a fixed constant and we are free to optimize ğ‘
to maximize: ğœ(ğ‘ + ğ‘) ğœ(ğ‘ âˆ’ ğ‘), we should set ğ‘ to zero.
Prove this theorem and give an interpretation in terms of log odds ratio and probability
